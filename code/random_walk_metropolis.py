"""
Provides several implementations of the random walk Metropolis (RWM) algorithm,
some homogeneous and some adaptive, each as both a single-chain and a multi-
chain sampler. Specifically, this module provides the following functions:
- rwm
- ada_rwm
- parallel_rwm
- parallel_ada_rwm
"""

import numpy as np
import numpy.linalg as alg
import numpy.random as rnd
import multiprocessing as mp
import time as tm
from sampling_utils import nrange
from parallel_plain_sampling import parallel_plain

def rwm(log_density, n_its, x_0, cov, gen, bar=False):
    """Runs the random walk Metropolis algorithm with Gaussian proposal
        distribution

        Args:
            log_density: log of the target density, must be a function taking
                a size d np array as input and returning a float representing
                the value of the log density at the given point
            n_its: number of iterations to run the algorithm for, must be non-
                negative integer
            x_0: initial state, must be size d np array describing a point 
                from the support of the target density
            cov: covariance parameter of the Gaussian proposal. There are three
                different ways to set this argument (the first two are slightly
                more computationally optimized for their respective cases).
                The interpretation of cov as a representation of the prior's
                covariance matrix depends on the type of cov as follows:
                - float: covariance is cov * np.identity(d)
                - 1d np array: covariance is np.diag(cov)
                - 2d np array: covariance is cov itself
                Accordingly, in the second case cov must be of size d and in the
                third case of shape (d,d).
            gen: instance of rnd.Generator to be used for pseudo-random number
                generation during sampling
            bar: bool denoting whether or not a progress bar should be displayed
                during the sampling procedure

        Returns:
            samples: np array of shape (n_its+1, d), where samples[i] is the 
                i-th sample generated by RWM
            gen: the generator given to the sampler in its latest state after
                the sampling performed in this call
            tde_cnts: an instance of np.ones(n_its+1), corresponding to the
                sampler's target density evaluation counts (only returned for
                compatibility with the parallel_plain_sampling module)
            runtimes: 1d np array of size n_its+1, where runtimes[i] is the
                time the chain took to perform its i-th iteration (in seconds)
            ldv: 1d np array of size n_its+1 giving the log density values of
                the samples
    """
    # prepare efficient proposal sampling
    if type(cov) == float or (type(cov) == np.ndarray and len(cov.shape) == 1):
        std = np.sqrt(cov)
        covify = lambda r: std * r
    elif type(cov) == np.ndarray and len(cov.shape) == 2:
        L = alg.cholesky(cov)
        covify = lambda r: L @ r
    else:
        raise TypeError("Invalid type for argument cov! Consult the docstring" \
            + " regarding valid choices.")
    # prepare sample and ldv storage
    d = x_0.shape[0]
    X = np.zeros((n_its+1,d))
    X[0] = x_0
    ldv = np.zeros(n_its+1)
    ldv[0] = log_density(X[0])
    runtimes = np.zeros(n_its+1)
    time_b = tm.time()
    # generate the samples
    for n in nrange(n_its, bar):
        x_nprop = covify(gen.normal(size=d)) + X[n-1]
        ldv_nprop = log_density(x_nprop)
        u = gen.uniform()
        if np.log(u) <= ldv_nprop - ldv[n-1]:
            X[n] = x_nprop
            ldv[n] = ldv_nprop
        else:
            X[n] = X[n-1]
            ldv[n] = ldv[n-1]
        time_a = tm.time()
        runtimes[n] = time_a - time_b
        time_b = time_a
    return X, gen, np.ones(n_its+1), runtimes, ldv

def ada_rwm(log_density, n_its, x_0, gen, bar=False):
    """Runs the adaptive random walk Metropolis algorithm proposed in Section 2
        of Roberts & Rosenthal's 2009 paper 'Examples of Adaptive MCMC'.

        Args:
            log_density: log of the target density, must be a function taking
                a size d np array as input and returning a float representing
                the value of the log density at the given point
            n_its: number of iterations to run the algorithm for, must be non-
                negative integer
            x_0: initial state, must be size d np array describing a point 
                from the support of the target density
            gen: instance of rnd.Generator to be used for pseudo-random number
                generation during sampling
            bar: bool denoting whether or not a progress bar should be displayed
                during the sampling procedure

        Returns:
            samples: np array of shape (n_its+1, d), where samples[i] is the 
                i-th sample generated by adaptive RWM
            gen: the generator given to the sampler in its latest state after
                the sampling performed in this call
            tde_cnts: an instance of np.ones(n_its+1), corresponding to the
                sampler's target density evaluation counts (only returned for
                compatibility with the parallel_plain_sampling module)
            runtimes: 1d np array of size n_its+1, where runtimes[i] is the
                time the chain took to perform its i-th iteration (in seconds)
            ldv: 1d np array of size n_its+1 giving the log density values for
                all samples
            m: np array of shape (n_its+1,d) containing all estimates of the
                target's mean throughout the run
            Sig: np array of shape (d,d) containing the final estimate of the
                target's covariance matrix
    """
    beta = 0.05 # 
    self_outer = lambda x: np.outer(x,x)
    # prepare sample, ldv and adaptation parameter storage (note that storing
    # all matrices used for the adaptation throughout the run would need an
    # exorbitant amount of memory)
    d = x_0.shape[0]
    inv_sqrt_d = 1 / np.sqrt(d)
    X = np.zeros((n_its+1,d))
    X[0] = x_0
    ldv = np.zeros(n_its+1)
    ldv[0] = log_density(X[0])
    m = np.zeros((n_its+1,d))
    m[0] = X[0]
    Q = self_outer(X[0])
    Sig = np.zeros((d,d))
    L = np.zeros((d,d))
    runtimes = np.zeros(n_its+1)
    time_b = tm.time()
    # generate the samples
    for n in nrange(n_its, bar):
        # generate proposal
        u = gen.uniform()
        if n <= 2*d:
            x_nprop = 0.1*inv_sqrt_d * gen.normal(size=d) + X[n-1]
        elif u <= beta or type(L) == type(None):
            x_nprop = 0.1*inv_sqrt_d * gen.normal(size=d) + X[n-1]
        else:
            x_nprop = 2.38*inv_sqrt_d * L @ gen.normal(size=d) + X[n-1]
        # accept or reject it
        ldv_nprop = log_density(x_nprop)
        u = gen.uniform()
        if np.log(u) <= ldv_nprop - ldv[n-1]:
            X[n] = x_nprop
            ldv[n] = ldv_nprop
        else:
            X[n] = X[n-1]
            ldv[n] = ldv[n-1]
        # adapt the proposal distribution's parameters
        m[n] = (n * m[n-1] + X[n]) / (n+1)
        Q = Q + self_outer(X[n])
        Sig = Q / n - (n+1)/n * self_outer(m[n])
        if n >= 2*d:
            try:
                L = alg.cholesky(Sig)
            except:
                L = None
        time_a = tm.time()
        runtimes[n] = time_a - time_b
        time_b = time_a
    return X, gen, np.ones(n_its+1), runtimes, ldv, m, Sig

def parallel_rwm(
        log_density,
        n_chains,
        n_its,
        x_0s,
        cov,
        verbose=True,
        bar=True
    ):
    """An implementation of the random walk Metropolis algorithm with Gaussian
        proposal that advances a number of chains in parallel (using CPU paral-
        lelization).

        Args:
            log_density: log of the target density, must be a function taking
                a size d np array as input and returning a float representing
                the value of the log density at the given point; must be pickle-
                able (i.e. it mustn't be a lambda function)
            n_chains: number of parallel chains to use, must be positive integer
            n_its: number of iterations to perform per chain, must be positive 
                integer
            x_0s: initial states for the parallel chains, must be 2d np array of 
                shape (n_chains,d)
            cov: covariance parameter of the Gaussian proposal. There are three
                different ways to set this argument (the first two are slightly
                more computationally optimized for their respective cases).
                The interpretation of cov as a representation of the prior's
                covariance matrix depends on the type of cov as follows:
                - float: covariance is cov * np.identity(d)
                - 1d np array: covariance is np.diag(cov)
                - 2d np array: covariance is cov itself
                Accordingly, in the second case cov must be of size d and in the
                third case of shape (d,d).
            verbose: bool denoting whether or not to print various status
                updates during the sampling procedure
            bar: bool denoting whether or not a progress bar should be displayed
                during the sampling procedure (for practical reasons, the 
                progress bar will display the progression of an arbitrary chain,
                not that of the slowest one)

        Returns:
            samples: np array of shape (n_its+1,n_chains,d), where 
                samples[i,j] is the i-th sample generated by the j-th chain
            runtimes: 2d np array of shape (n_its+1,n_chains), where
                runtimes[i,j] is the time the j-th chain took to perform its
                i-th iteration (in seconds)
    """
    pp_ret = parallel_plain(
        rwm,
        log_density,
        n_chains,
        n_its,
        x_0s,
        cov,
        verbose,
        bar
    )
    return pp_ret[0], pp_ret[2]

def parallel_ada_rwm(
        log_density,
        n_chains,
        n_its,
        x_0s,
        verbose=True,
        bar=True
    ):
    """An implementation of the adaptive random walk Metropolis algorithm
        proposed in Section 2 of Roberts & Rosenthal's 2009 paper 'Examples of
        Adaptive MCMC' that advances a number of independent chains in parallel
        (using CPU parallelization).

        Args:
            log_density: log of the target density, must be a function taking
                a size d np array as input and returning a float representing
                the value of the log density at the given point; must be pickle-
                able (i.e. it mustn't be a lambda function)
            n_chains: number of parallel chains to use, must be positive integer
            n_its: number of iterations to perform per chain, must be positive 
                integer
            x_0s: initial states for the parallel chains, must be 2d np array of 
                shape (n_chains,d)
            verbose: bool denoting whether or not to print various status
                updates during the sampling procedure
            bar: bool denoting whether or not a progress bar should be displayed
                during the sampling procedure (for practical reasons, the 
                progress bar will display the progression of an arbitrary chain,
                not that of the slowest one)

        Returns:
            samples: np array of shape (n_its+1,n_chains,d), where 
                samples[i,j] is the i-th sample generated by the j-th chain
            runtimes: 2d np array of shape (n_its+1,n_chains), where
                runtimes[i,j] is the time the j-th chain took to perform its
                i-th iteration (in seconds)
            ldv: 2d np array of shape (n_its+1,n_chains) giving the log density
                values for all samples of all chains
            m: np array of shape (n_its+1,n_chains,d) containing all estimates
                of the target's mean by each chain throughout the run
            Sig: np array of shape (n_chains,d,d) containing the final estimate
                of the target's covariance matrix by each chain
    """
    if verbose:
        print("Checking validity of given arguments...")
    if type(n_chains) != int or n_chains <= 0:
        raise TypeError("Number of parallel chains must be positive integer!")
    if type(n_its) != int or n_its <= 0:
        raise TypeError("Number of iterations must be positive integer!")
    if type(x_0s) != np.ndarray or len(x_0s.shape) != 2 \
        or x_0s.shape[0] != n_chains:
        raise TypeError("Initial values must be given as 2d np array of shape" \
            + " (n_chains,d)!")
    if verbose:
        print("Preparing for parallel sampling...")
    d = x_0s.shape[-1]
    # initialize generators for random numbers
    seeds = rnd.SeedSequence().spawn(n_chains)
    gens = [rnd.Generator(rnd.MT19937(s)) for s in seeds]
    # prepare arguments for parallelized call
    bars = [bar] + (n_chains - 1) * [False]
    starmap_args = [(log_density, n_its, x_0, gen, b)
                    for x_0, gen, b in zip(x_0s, gens, bars)]
    # run parallel chains and process the returns
    if verbose:
        print("Starting parallel sampling...")
    pool = mp.Pool(n_chains)
    returns = pool.starmap(ada_rwm, starmap_args)
    pool.close()
    if verbose:
        print("Processing returns and terminating...")
    samples = np.transpose(np.array([ret[0] for ret in returns]), axes=(1,0,2))
    runtimes = np.array([ret[3] for ret in returns]).T
    ldvs = np.array([ret[4] for ret in returns]).T
    means = np.transpose(np.array([ret[5] for ret in returns]), axes=(1,0,2))
    covs = np.array([ret[6] for ret in returns])
    return samples, runtimes, ldvs, means, covs

